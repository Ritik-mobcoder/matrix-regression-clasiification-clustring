{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = sns.load_dataset(\"iris\")\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "df = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"species\"])  \n",
    "y = df[\"species\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['versicolor', 'setosa', 'virginica', 'versicolor', 'versicolor',\n",
       "       'setosa', 'versicolor', 'virginica', 'versicolor', 'versicolor',\n",
       "       'virginica', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor',\n",
       "       'virginica', 'versicolor', 'versicolor', 'virginica', 'setosa',\n",
       "       'virginica', 'setosa', 'virginica', 'virginica', 'virginica',\n",
       "       'virginica', 'virginica', 'setosa', 'setosa', 'setosa', 'setosa',\n",
       "       'versicolor', 'setosa', 'setosa', 'virginica', 'versicolor',\n",
       "       'setosa', 'setosa', 'setosa', 'virginica', 'versicolor',\n",
       "       'versicolor', 'setosa', 'setosa'], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.97, 0.03],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.02, 0.98],\n",
       "       [0.  , 0.99, 0.01],\n",
       "       [0.  , 0.92, 0.08],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.07, 0.93],\n",
       "       [0.  , 0.85, 0.15],\n",
       "       [0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.09, 0.91],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.95, 0.05, 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.87, 0.13],\n",
       "       [0.  , 0.  , 1.  ],\n",
       "       [0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.98, 0.02],\n",
       "       [0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.07, 0.93],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.02, 0.98],\n",
       "       [0.  , 0.02, 0.98],\n",
       "       [0.  , 0.03, 0.97],\n",
       "       [0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.01, 0.99, 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.06, 0.94],\n",
       "       [0.  , 0.98, 0.02],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.05, 0.95],\n",
       "       [0.02, 0.9 , 0.08],\n",
       "       [0.  , 1.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [1.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Confusion Matrix\n",
    "A **confusion matrix** is a table that describes the performance of a classification model on a set of data for which the true values are known. It shows the counts of:\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive cases.\n",
    "- **True Negatives (TN)**: Correctly predicted negative cases.\n",
    "- **False Positives (FP)**: Incorrectly predicted as positive.\n",
    "- **False Negatives (FN)**: Incorrectly predicted as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric\n",
    "\n",
    "**Accuracy** is a commonly used evaluation metric for classification models. It measures the proportion of correctly classified instances (both true positives and true negatives) out of the total number of instances in the dataset.\n",
    "\n",
    "The formula for calculating accuracy is:\n",
    "\n",
    "\\[\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **TP (True Positives)**: Correctly predicted positive instances.\n",
    "- **TN (True Negatives)**: Correctly predicted negative instances.\n",
    "- **FP (False Positives)**: Incorrectly predicted as positive.\n",
    "- **FN (False Negatives)**: Incorrectly predicted as negative.\n",
    "\n",
    "Accuracy provides a simple way to assess how well the model is performing, but it can be misleading if the dataset is imbalanced (i.e., one class is much more frequent than the other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Metric\n",
    "\n",
    "**Precision** is a metric used to evaluate the accuracy of positive predictions. It is the ratio of correctly predicted positive observations to the total predicted positive observations. Precision answers the question: *Of all the instances the model predicted as positive, how many are actually positive?*\n",
    "\n",
    "Precision is especially important when the cost of false positives is high. For example, in email spam detection, we want to minimize false positives (non-spam emails marked as spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Weighted): 1.0\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred, average='weighted') \n",
    "print(f\"Precision (Weighted): {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** (also known as **sensitivity** or **true positive rate**) measures the proportion of actual positive instances that are correctly identified by the model. It answers the question: *Of all the instances that are actually positive, how many did the model correctly classify?*\n",
    "\n",
    "The formula for recall is:\n",
    "\n",
    "\\[\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- **TP (True Positives)**: Correctly predicted positive instances.\n",
    "- **FN (False Negatives)**: Incorrectly predicted as negative.\n",
    "\n",
    "Recall is crucial when the cost of false negatives is high, such as in medical diagnoses where missing a positive case (e.g., cancer detection) could be dangerous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (Weighted): 1.0\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall (Weighted): {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted') \n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **F1 Score** is the harmonic mean of **precision** and **recall**. It provides a balance between the two metrics and is particularly useful when the class distribution is imbalanced. The formula for F1 score is:\n",
    "\n",
    "\\[\n",
    "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "\n",
    "The F1 score helps to balance the trade-off between precision and recall. It is especially useful when you need to account for both false positives and false negatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 1.0\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score (Weighted): {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC AUC** is a metric that evaluates how well the classifier distinguishes between classes. The **Receiver Operating Characteristic (ROC)** curve plots the **True Positive Rate (Recall)** against the **False Positive Rate**. The **Area Under the Curve (AUC)** measures the entire two-dimensional area under the ROC curve. The AUC value ranges from 0 to 1:\n",
    "- **AUC = 1**: Perfect model that perfectly distinguishes between classes.\n",
    "- **AUC = 0.5**: The model performs no better than random guessing.\n",
    "- **AUC < 0.5**: The model performs worse than random guessing.\n",
    "\n",
    "The **multi-class AUC** can be computed with the \"one-vs-rest\" (OvR) method for multi-class classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
    "print(f\"ROC-AUC: {roc_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
